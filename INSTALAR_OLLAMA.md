# ğŸ¤– Como Instalar Ollama (IA Gratuita e Local)

## âš¡ Por Que Ollama?

- âœ… **100% GRATUITO** (sem limites de uso)
- âœ… **Funciona OFFLINE** (nÃ£o precisa de internet)
- âœ… **PRIVADO** (seus dados nÃ£o saem do PC)
- âœ… **RÃPIDO** (depois do download inicial)
- âœ… **SEM QUOTAS** (use quanto quiser)

**Vs. OpenAI:** Sua conta atingiu o limite e cobra por uso  
**Vs. Gemini:** Pode ter instabilidade e limites de requisiÃ§Ãµes  
**Ollama:** Sem limites, sem custos, sempre funciona!

---

## ğŸš€ InstalaÃ§Ã£o (5 minutos)

### Passo 1: Baixar Ollama

**Windows:**
1. Acesse: https://ollama.com/download
2. Clique em "Download for Windows"
3. Execute o instalador `OllamaSetup.exe`
4. Siga o assistente (Next â†’ Next â†’ Install)

**Outras plataformas:**
- Mac: `brew install ollama`
- Linux: `curl https://ollama.ai/install.sh | sh`

### Passo 2: Baixar um Modelo

Abra o **PowerShell** ou **CMD** e execute:

```bash
ollama run llama3.1
```

**O que acontece:**
- â¬‡ï¸ Download do modelo (~4.7 GB)
- â³ Leva 5-15 minutos dependendo da internet
- âœ… ApÃ³s download, o modelo fica pronto

**Primeira vez:**
```
pulling manifest
pulling 8eeb52dfb3bb... 100% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 4.7 GB
verifying sha256 digest
writing manifest
success
```

**Teste:**
```
>>> Hello
OlÃ¡! Como posso ajudar?

>>> /bye
```

### Passo 3: Deixar Rodando

Ollama roda como **serviÃ§o em background** automaticamente.

**Verificar se estÃ¡ rodando:**
```bash
ollama list
```

Deve mostrar:
```
NAME            SIZE
llama3.1:latest 4.7 GB
```

**Status do servidor:**
```bash
# Acessar no navegador
http://localhost:11434
```

Deve mostrar: `Ollama is running`

---

## âœ… Pronto! Agora Use no App

### No TCC Gado Gordo:

```bash
# 1. Reiniciar o app
streamlit run app.py

# 2. Ir para "Assistente IA"
# 3. Agora vai funcionar!
```

**O que muda:**
- âœ… Chat IA funciona
- âœ… Resumo automÃ¡tico funciona
- âœ… Respostas em portuguÃªs
- âœ… SEM CUSTO, SEM LIMITES

---

## ğŸ§ª Testar Ollama

### Teste no terminal:

```bash
ollama run llama3.1
>>> Qual a diferenÃ§a entre gado gordo e gado magro?
```

Deve responder em portuguÃªs explicando!

```bash
>>> /bye
```

### Teste via API:

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.1",
  "prompt": "OlÃ¡, vocÃª fala portuguÃªs?",
  "stream": false
}'
```

---

## ğŸ”§ Comandos Ãšteis

### Ver modelos instalados:
```bash
ollama list
```

### Baixar outro modelo:
```bash
ollama pull llama3.2        # Mais leve (3B)
ollama pull mistral         # Alternativa
ollama pull codellama       # Para cÃ³digo
```

### Remover modelo:
```bash
ollama rm llama3.1
```

### Parar servidor (se necessÃ¡rio):
```bash
# Windows: Task Manager â†’ Ollama â†’ Finalizar
```

### Iniciar servidor manualmente:
```bash
ollama serve
```

---

## âš™ï¸ ConfiguraÃ§Ã£o AvanÃ§ada (Opcional)

### Arquivo `.env` do TCC Gado Gordo:

```env
# Priorizar Ollama
PREFERRED_AI_MODEL=ollama

# ConfiguraÃ§Ãµes Ollama (padrÃ£o jÃ¡ funciona)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1

# Deixe OpenAI vazio se excedeu quota
OPENAI_API_KEY=

# Gemini como fallback (se tiver chave vÃ¡lida)
GEMINI_API_KEY=sua_chave_gemini
```

---

## ğŸ’¡ Dicas

### Primeira Vez
1. Execute `ollama run llama3.1` (baixa e testa)
2. Saia com `/bye`
3. Servidor continua rodando em background
4. Use no app normalmente

### Performance
- **Primeira resposta:** ~5-10 segundos (carrega modelo na RAM)
- **Respostas seguintes:** ~2-3 segundos (modelo jÃ¡ carregado)
- **RAM necessÃ¡ria:** 8GB recomendado (4GB mÃ­nimo)

### Modelos Alternativos

**Mais leve (PCs fracos):**
```bash
ollama run llama3.2:1b    # 1.3 GB, rÃ¡pido
```

**PortuguÃªs otimizado:**
```bash
ollama run gemma2:2b      # Bom em PT-BR
```

**Melhor qualidade:**
```bash
ollama run llama3.1:70b   # 40 GB, muito melhor
```

---

## ğŸ†˜ SoluÃ§Ã£o de Problemas

### "Command not found: ollama"

**SoluÃ§Ã£o Windows:**
1. Feche e reabra o PowerShell
2. Ou reinicie o computador
3. PATH deve ser atualizado automaticamente

### "Error: connection refused"

**SoluÃ§Ã£o:**
```bash
ollama serve
```

Deixe este terminal aberto.

### "Model not found"

**SoluÃ§Ã£o:**
```bash
ollama pull llama3.1
```

### App nÃ£o detecta Ollama

**VerificaÃ§Ãµes:**
1. `http://localhost:11434` deve responder "Ollama is running"
2. `ollama list` deve mostrar llama3.1
3. Reinicie o Streamlit

---

## âœ… Checklist

ApÃ³s instalaÃ§Ã£o:
- [ ] `ollama --version` mostra versÃ£o
- [ ] `ollama list` mostra llama3.1
- [ ] http://localhost:11434 responde
- [ ] `ollama run llama3.1` funciona
- [ ] Streamlit detecta (veja "IA Ativa: OLLAMA")
- [ ] Chat funciona no app

---

## ğŸ‰ Vantagens do Ollama

| Recurso | OpenAI | Gemini | Ollama |
|---------|--------|--------|--------|
| Custo | Pago | GrÃ¡tis* | GrÃ¡tis |
| Limites | Quota | RPM | Nenhum |
| Internet | Sim | Sim | NÃ£o |
| Privacidade | Nuvem | Nuvem | Local |
| Velocidade | RÃ¡pido | MÃ©dio | RÃ¡pido** |
| InstalaÃ§Ã£o | API key | API key | Download |

*Gemini tem limites de requisiÃ§Ãµes por minuto  
**Depois do download inicial

---

## ğŸ“ Suporte Ollama

- **DocumentaÃ§Ã£o:** https://ollama.com/docs
- **Modelos:** https://ollama.com/library
- **GitHub:** https://github.com/ollama/ollama

---

## ğŸš€ InstalaÃ§Ã£o RÃ¡pida (Resumo)

```bash
# 1. Baixar e instalar
https://ollama.com/download

# 2. Executar modelo
ollama run llama3.1

# 3. Aguardar download (~5-15 min)

# 4. Testar
>>> Ola, voce fala portugues?

# 5. Sair
>>> /bye

# 6. Usar no app
streamlit run app.py
```

**Pronto! IA gratuita e ilimitada!** ğŸ‰

---

**RecomendaÃ§Ã£o:** Use Ollama como padrÃ£o e deixe OpenAI/Gemini apenas como backup.

**Tempo total:** 20 minutos (incluindo download)  
**Custo:** R$ 0,00  
**Limite de uso:** Infinito âˆ

